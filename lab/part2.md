### Training the gym environment using an RL framework (TorchRL, rlmeta)

Now that we have shown how an gym environment is constructed, we need to train an RL agent to interact with the environment. There are many existing RL frameworks that are compatible with the standard gym environment, here are some notable examples:

* [TorchRL](https://pytorch.org/rl/), torchRL is part of the famous pytorch framework, development lead by Vincent Moens from Meta.
* [RLLib/Ray](https://docs.ray.io/en/latest/rllib/index.html)
* [Rlmeta](https://github.com/facebookresearch/rlmeta), also developed by Meta.

These frameworks have their individual strengths and reach different sweet points in terms of usability, modifiabilty and efficiency. However, for the cache guessing game, all of them works for this purpose.

Since we work for or work closely with Meta and some of us has been involved in the development of these frameworks, we provide both the script from both TorchRL and RLMeta. For TorchRL, the related torchRL script are located in ```src/torchrl/```. For RLMeta, the related script are located in ```src/rlmeta/```. 

### Train the RL agent using TorchRL

```src/torchrl/train_ppo_attack.py``` is the script for training. The default configuration for training is specified in ```src/torchrl/config/ppo_attack.yml```. The environment configuration is specified in ```src/torchrl/config/env_config/``` and the model configuration of the policy net is specified in 
```src/torchrl/config/model_config/```.

Please use the following to launch the training

```
(py38) $ python ${REPO_ROOT}/src/torchrl/train_ppo_attack.py
```

select (2) when prompted to plot all the training progress in wandb. 

On your browser, open a new tab and open ```https://wandb.ai``` and navigate to the ```rl4cache``` project, where you can see the current training and testing return and other statistics. As the training goes, the return will gradually increase to a positive value close to 1.0, which indicates that the RL agent (attacker) will be able to guess the secret with high accuracy. (Since the correct guess reward is 1.0 and wrong guess reward is -1.0, only if most of the guesses are correct will the reward be close to 1.0). The script will also automatically save the checkpoint of the model of the policy net in ```src/torchrl/output``` directory.

### Sampling the policy net to get the attack trajectory using TorchRL

Once we have trained the policy net to have very high guess correct rate, we want to get a sample of the attack trajectory generated by this policy net, this can be done using the following command.

```
(py38) $ python ${REPO_ROOT}/src/torchrl/sampling_trajectory.py ${PATH_TO_CHECKPOINT}
```
Which it will generate the attack sequence.

### Train the RL agent using RLMeta

```src/rlmeta/train_ppo_attack.py``` is the script for training. The default configuration for training is specified in ```src/rlmeta/config/ppo_attack.yml```. The environment configuration is specified in ```src/torchrl/config/env_config/``` and the model configuration of the policy net is specified in 
```src/rlmeta/config/model_config/```.

Please use the following to launch the training

```
(py38) $ python ${REPO_ROOT}/src/rlmeta/train_ppo_attack.py env_config=<PATH_TO_ENV_CONFIG>
```

Currently RLMeta script is not integrated with Wandb, the training progress is plotted directly on the terminal.

### Sampling the policy net to get the attack trajectory using RLMeta

Once we have trained the policy net to have very high guess correct rate, we want to get a sample of the attack trajectory generated by this policy net, this can be done using the following command.

```
(py38) $ python ${REPO_ROOT}/src/rlmeta/sample_attack.py ${PATH_TO_CHECKPOINT}
```
Which it will generate the attack sequence.








